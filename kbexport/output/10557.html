<h1>SQL Server Performance Issues   Buffer latch error</h1><h2>10557</h2><h3>Introduction</h3><p>this article explains the basic troubleshooting tips for sql server performance and variety of sql jobs / t-sql performance improvement techniques.</p><h3>Symptom</h3><p>a) Executed as user: NT AUTHORITY\SYSTEM. Time-out occurred while waiting for buffer latch type 3 for page (1:0), database ID 7. [SQLSTATE 42000] (Error 845).&nbsp; The step failed.</p><h3>Cause</h3><p></p><h3>Workaround</h3><p></p><h3>Resolution</h3><p><P>go to this site to get&nbsp;answers for all SQL Server performance related issues <A href="http://www.sql-server-performance.com">http://www.sql-server-performance.com</A>.&nbsp;</P><P>i found this link <A href="http://www.sql-server-performance.com/misc_tips.asp">http://www.sql-server-performance.com/misc_tips.asp</A>&nbsp; is more useful and had lot of tips for solving different t-sql / back up related issues. i&nbsp;am also pasting contents of this&nbsp;page in case if this is moved or not found in future.&nbsp;</P><H1 class=pageTitle>Miscellaneous SQL Server Performance Tuning Tips</H1><HR SIZE=2><P><BR><BR>&nbsp;</P><P><STRONG>When your transaction log grows too large and you don't care for its contents</STRONG> a quick way to shrink it is to change the database recovery mode to "simple" and truncate log files. Simple recovery mode will force SQL Server to keep only the "active" portion of the log, which is very small. After you have truncated your log file(s), you can switch back to your original recovery mode. [7.0, 2000] Contributed by Tom Kitta. <EM>Added 1-10-2006</EM></P><P class=asterisk>*****</P><P><STRONG>If you need to delete all the rows in a table</STRONG>, don't use DELETE to delete them all, as the DELETE statement is a logged operation and can take time. To perform the same task much faster, use the TRUNCATE TABLE instead, which is not a logged operation. Besides deleting all of the records in a table, this command will also reset the seed of any IDENTITY column back to its original value.</P><P>After you have run the DELETE statement, it is important then to manually update the statistics on this table using UPDATE STATISTICS. This is because running TRUNCATE TABLE will not reset the statistics for the table, which means that as you add data to the table, the statistics for that table will be incorrect. Of course, if you wait long enough, and if you have Auto Update Statistics turned on for the database, then the statistics will eventually catch up with themselves. But this may not happen quickly, resulting in slowly performing queries because the Query Optimizer is using incorrect statistics. [6.5, 7.0, 2000] <EM>Updated 2-16-2004</EM></P><P class=asterisk>*****</P><P>DELETE operations can sometimes be time- and space-consuming. In some environments you might be able to increase the performance of this operation when using TRUNCATE instead of DELETE. TRUNCATE will almost instantly be executed. However, TRUNCATE will not work when there are Foreign Key references present for that table. A workaround is to DROP the constraints before firing the TRUNCATE. Here's a generic script that will drop all existing Foreign Key constraints on a specific table:</P><P class=textCode>CREATE TABLE dropping_constraints <BR>( <BR>cmd VARCHAR(8000) <BR>) <BR><BR>INSERT INTO dropping_constraints <BR>SELECT <BR>'ALTER TABLE [' + <BR>t2.Table_Name + <BR>'] DROP CONSTRAINT ' + <BR>t1.Constraint_Name <BR>FROM <BR>INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS t1 <BR>INNER JOIN <BR>INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE t2 <BR>ON <BR>t1.CONSTRAINT_NAME = t2.CONSTRAINT_NAME <BR>WHERE t2.TABLE_NAME='your_tablename_goes_here' <BR>DECLARE @stmt VARCHAR(8000) <BR>DECLARE @rowcnt INT <BR>SELECT TOP 1 @stmt=cmd FROM dropping_constraints <BR>SET @rowcnt=@@ROWCOUNT <BR>WHILE @rowcnt&lt;&gt;0 <BR>BEGIN <BR>EXEC (@stmt) <BR>SET @stmt = 'DELETE FROM dropping_constraints WHERE cmd ='+ QUOTENAME(@stmt,'''') <BR>EXEC (@stmt) <BR>SELECT TOP 1 @stmt=cmd FROM dropping_constraints <BR>SET @rowcnt=@@ROWCOUNT <BR>END <BR>DROP TABLE dropping_constraints</P><P>This can also be extended to drop all FK constraints in the current database. To achieve this, just comment the WHERE clause. [7.0, 2000] <EM>Added 6-3-2005</EM></P><P class=asterisk>*****</P><P><STRONG>Don't run a screensaver on your production SQL Server</STRONG>, it can unnecessarily use CPU cycles that should be going to your application. The only exception to this is the "blank screen" screensaver, which is OK to use. [6.5, 7.0, 2000] <EM>Updated 2-16-2004</EM></P><P class=asterisk>*****</P><P><STRONG>Don't run SQL Server on the same physical server that you are running Terminal Services, or Citrix software</STRONG>. Both Terminal Services and Citrix are huge resource hogs, and will significantly affect the performance of SQL Server. Running the administrative version of Terminal Services on a SQL Server physical server, on the other hand, is OK, and a good idea from a convenience point of view. As is mentioned in other parts of this website, ideally, SQL Server should run on a dedicated physical server. But if you have to share a SQL Server with another application, make sure it is not Terminal Services or Citrix. [7.0, 2000] <EM>Updated 2-16-2004</EM></P><P class=asterisk>*****</P><P>Use <STRONG>sp_who and sp_who2</STRONG> (sp_who2 is not documented in the SQL Server Books Online, but offers more details than sp_who) to provide locking and performance-related information about current connections to SQL Server. Sometimes, when SQL Server is very busy, you can't use Enterprise Manager to view current connection activity, but you can always use these two commands from Query Analyzer, even when SQL Server is very busy. [6.5, 7.0, 2000] <EM>Updated 2-16-2004</EM></P><P class=asterisk>*****</P><P><STRONG>SQL Server 7.0 and 2000 use its own internal thread scheduler</STRONG> (called the UMS) when running in either native thread mode or in fiber mode. By examining the UMS's Scheduler Queue Length, you can help determine whether or not that the CPU or CPUs on the server are presenting a bottleneck.</P><P>This is similar to checking the System Object: Processor Queue Length in Performance Monitor. If you are not familiar with this counter, what this counter tells you is how many threads are waiting to be executed on the server. Generally, if there are more than two threads waiting to be executed on a server, then that server can be assumed to have a CPU bottleneck.</P><P>The advantage of using the UMS's Schedule Queue Length over the System Object: Processor Queue Length is that it focuses strictly on SQL Server threads, not all of the threads running on a server.</P><P>To view what is going on inside the UMS, you can run the following undocumented command:</P><P>DBCC SQLPERF(UMSSTATS)</P><P>For every CPU in your server, you will get Scheduler. Each Scheduler will be identified with a number, starting with 0. So if you have four CPUs in your server, there will be four Schedulers listed after running the above command, Schedulers ID 0 through 3.</P><P>The "num users" tells you the number of SQL threads there are for a specific scheduler.</P><P>The "num runnable," or better known as the "Scheduler Queue Length," is the key indicator to watch. Generally, this number will be 0, which indicates that there are no SQL Server threads waiting to run. If this number is 2 or more, this indicates a possible CPU bottleneck on the server. Keep in mind that the values presented by this command are point data, which means that the values are only accurate for the split second when they were captured, and will be always changing. But if you run this command when the server is very busy, the results should be indicative of what is going on at that time. You may want to run this command multiple time to see what is going on over time.</P><P>The "num workers" refers to the actual number of worker threads there are in the thread pool.</P><P>The "idle workers" refers to the number of idle worker threads.</P><P>The "cntxt switches" refers to the number of context switches between runnable threads.</P><P>The "cntxt switches(idle)" refer to the number of context switches to "idle" threads.</P><P>As you can see, this command is for advanced users, and is just one of many tools that can be used to see internally how SQL Server is performing. [7.0, 2000] <EM>Updated 8-29-2003</EM></P><P class=asterisk>*****</P><P><STRONG>SQL Server 2000 offers a system table-valued function that provides statistical information on the I/O activity of specific database files</STRONG>. It is called fn_virtualfilestats. What is interesting about this function is that it is specific to a database file. For example, if you want to see the I/O activity of a particular user database, you can. Keep in mind that all SQL Server databases have at least two physical files (for the database and log), and can have many more, depending on how the database was created. When using this function, you have to not only specify the database, but the file within the database. This is very specific I/O statistics indeed. To run this function, use the syntax found below:</P><P class=textCode>SELECT * FROM :: fn_virtualfilestats(<EM>dbid</EM>, <EM>fileid</EM>)</P><P><EM>Where: <BR><BR>dbid</EM>: The database ID of the database you want to examine.</P><P><EM>fileid</EM>: The file ID of the physical files that make up your database. At a minimum, each database has at least two files: the database file (MDF file) and a log file (LDF). A database can have many files, and the file number refers to the number of the physical file that you want to examine.</P><P>To identify the dbid and fileid you want to use in the above statement, run the following SELECT statement. It will provide you with a list of all of the database names, database ids, and file ids (for each database) on your server.</P><P class=textCode>SELECT sysdatabases.name AS Database_Name, <BR>sysdatabases.dbid AS Database_ID, <BR>sysaltfiles.fileid AS File_ID <BR>FROM sysaltfiles INNER JOIN sysdatabases <BR>ON sysaltfiles.dbid = sysdatabases.dbid <BR>ORDER BY sysdatabases.name, sysaltfiles.fileid</P><P>Once you run this function, a wide variety of information is available, including:</P><P><EM>NumberReads</EM>: The number of physical reads made against this file since the last time SQL Server was restarted.</P><P><EM>NumberWrites</EM>: The number of physical writes made against this file since the last time SQL Server was restarted.</P><P><EM>BytesRead</EM>: The number of bytes read from this file since the last time SQL Server was restarted.</P><P><EM>BytesWritten</EM>: The number of writes to this file since the last time SQL Server was restarted.</P><P><EM>IoStallMS</EM>: The total amount of time that users have waited for I/Os to complete for this file (in milliseconds).</P><P>The first four statistics can give you a feel for how busy a particular file is. This can come in handy when comparing multiple filegroups in a database and to see how balanced the I/O is to each file. To make the most of filegroups, I/O should be spread out among the various files for best overall performance. The last statistic, IoStallMS, is best used find out if your have a bottleneck in your transaction log, as demonstrated below:</P><P class=textCode>SELECT IoStallMS / (NumberReads+NumberWrites) as IsStall <BR>FROM :: fn_virtualfilestats(<EM>dbid</EM>, <EM>fileid</EM>)</P><P><EM>Where: <BR><BR>dbid</EM>: The database ID of the database you want to examine.</P><P><EM>fileid</EM>: The file ID of the transaction log of the database being examined.</P><P>Essentially, if IsStall is &gt; 20ms, then this indicates that the I/O to the transaction log is becoming a bottleneck, which in turn can lead to major concurrently problems in the database.</P><P>To help optimize the transaction log, assuming it has become a bottleneck, consider doing the following:</P><UL><LI>Place the transaction log on a faster disk array. <LI>Place the transaction log on a dedicated disk array (no other files other than the transaction log). This allows sequential writes to occur as fast as possible, helping to boost I/O performance. <LI>Turn on write-back caching on your disk controller, but only if it is backed up with a battery and has been tested to work with SQL Server. Normally, write-back caching is turned off because it can cause database corruption should the server crash. </LI></UL><P>[2000] <EM>Updated 2-16-2004</EM></P><P class=asterisk>*****</P><P><STRONG>Sometimes, a user thread has to wait until the resources it needs are available</STRONG>. Hopefully, this won't happen often, but it is a fact of life. But sometimes, long waits can indicate potential performance problems than can be corrected, if you know where to look. Long waits can be caused by blocking locks, slow I/O, and other factors.</P><P>Fortunately, you can access the amount of time a user thread has to wait, which can tell you which user thread, if any, is taking more time that it should. For example, the query below can be run to identify any user threads that have to wait more than one second:</P><P class=textCode>SELECT spid, waittime, lastwaittype <BR>FROM master..sysprocesses <BR>WHERE waittime &gt; 1000</P><P>When you run the above query, all of the processes that have been waiting for greater than 1 second (1000 milliseconds)--both system and user--will be displayed. You will want to ignore system processes, focusing your efforts on spids that represent specific user threads. Spid will give you the user ID (you will have to match the spid to the appropriate user), waittime is the number of milliseconds that this user thread has been waiting, and lastwaittype will give you a clue as to what is waiting to occur.</P><P>In the above query, I have used 1000 milliseconds as a cutoff point, but you can use any amount of time you want. Generally speaking, any user thread that has been waiting for 5 seconds or more should definitely be evaluated for potential performance issues. Of course, you can choose any threshold you want. [7.0 2000] <EM>Updated 8-29-2003</EM></P><P class=asterisk>*****</P><P><STRONG>By default, you cannot use a UNC (Universal Naming Convention) name</STRONG> to specify a location of where to store a SQL Server database or log file. Instead, you must specify a drive letter that refers to a local physical drive or array. But what if you want to store your database or log file on another NT Server or a <A href="http://www.netapp.com/">Network Appliance, Inc</A>. storage system? You can, but you will have to set Trace Flag 1807 on your SQL Server to allow the use of UNC names. [7.0]</P><P class=asterisk>*****</P><P><STRONG>For a quick and dirty way to check to see if your SQL Server has maxed out its memory</STRONG> (and causing your server to page), try this. Bring up the Task Manager and go to the "Performance" tab.</P><P>Here, check out two numbers: the "Total" under "Commit Charge (k)" and the "Total" under "Physical Memory (k)". If the "Total" under "Commit Charge (k)" is greater than the "Total" under "Physical Memory (k)", then your server does not have enough physical memory to run efficiently as it is currently configured and is most likely causing your server to page unnecessarily. Excess paging will slow down your server's performance.</P><P>Another number to make note of is the "Available Physical Memory (K). This number should be 4MB or higher. If it is not, then your SQL Server is most likely suffering from a lack of physical RAM, hurting performance, and more RAM needs to be added.</P><P>If you notice this problem, you will probably want to use Performance Monitor to further investigate the cause of this problem. You will also want to check to see how much physical memory has been allocated to SQL Server. Most likely, this setting has been set incorrectly, and SQL Server has been set to use too much physical memory. Ideally, SQL Server should be set to allocate physical RAM dynamically. [7.0, 2000] <EM>Updated 1-29-2003</EM></P><P class=asterisk>*****</P><P>When performance tuning a SQL Server, <STRONG>it is often handy to know if the disk I/O of your servers (and the databases on it) are mostly reads or mostly writes</STRONG>. This information can be used to calculate the ratio of writes to reads of your server, and this ratio can affect how you might want to tune your SQL Server. For example, if you find that your server is heavy on the writes, then you will want to avoid RAID 5 if you can, and use RAID 10 instead. This is because RAID 5 is much less efficient that RAID 10 at writes. But if your server has a much greater number of reads than writes, then perhaps a RAID 5 system is more than adequate. <BR><BR>One of the quickest ways to find out the ratio of reads to writes on your SQL Servers is to run Task Manager and look at the sqlservr.exe process (this is the mssqlserver service) and view the total number of I/O Read Bytes and I/O Write Bytes. If you don't see this in Task Manager, go to View|Select Column, and add these two columns to Task Manager. <BR><BR>The results you see tell you how many bytes of data have been written and read from the SQL Server service since it was last restarted. Because of this, you don't want to read this figure immediately after starting the SQL Server service, but after several days of typical use. <BR><BR>In one particular case I looked at, the SQL Server had 415,006,801,908 I/O bytes read and 204,669,746,458 bytes written. In this case, this server has about one write for every two reads. In this case, RAID 5 is probably a good compromise in performance, assuming that RAID 10 is not available from a budget perspective. But if the reverse were true, and there was two writes for every one read, then RAID 10 would be needed for best overall performance of SQL Server. [6.5, 7.0, 2000] <EM>Added 1-29-2003</EM></P><P class=asterisk>*****</P><P><STRONG>Internet Information Server (IIS) has the ability to send its log files directly to SQL Server for storage</STRONG>. Busy IIS servers can actually get bogged down trying to write log information directly to SQL Server, and because of this, it is generally not recommend to write a web logging information to SQL Server. Instead, logs should be written to text files, and later imported into SQL Server using BCP or DTS. [6.5, 7.0, 2000]</P><P class=asterisk>*****</P><P>SQL Server 2000 has a <STRONG>database compatibility mode</STRONG> that allows applications written for previous versions of SQL Server to run under SQL Server 2000. In you want maximum performance for your database, you don't want to run your database in compatibility mode (not all new performance-related features are supported).</P><P>Instead, your databases should be running in native SQL Server 2000 mode. Of course, this may require you to modify your application to make it SQL Server 2000 compliant, but in most cases, the additional work required to update your application will be more than paid for with improved performance.</P><P>SQL Server 7.0 compatibility level is referred to as "70" and SQL Server 2000 compatibility level is referred to as "80". [7.0, 2000] <EM>Updated 9-06-2004</EM></P><P class=asterisk>*****</P><P><STRONG>When experimenting with the tuning of your SQL Server</STRONG>, you may want to run the DBCC DROPCLEANBUFFERS command to remove all the test data from SQL Server's data cache (buffer) between tests to ensure fair testing. Keep in mind that this command only removes clean buffers, not dirty buffers. Because of this, before running the DBCC DROPCLEANBUFFERS command, you may first want to run the CHECKPOINT command first. Running CHECKPOINT will write all dirty buffers to disk. And then when you run DBCC DROPCLEANBUFFERS, you can be assured that all data buffers are cleaned out, not just the clean ones.</P><P>If you want to clear out all of the stored procedure cache, use this command, DBCC FREEPROCCACHE. If you want to only clear out the stored procedure cache for a single database (not the entire server) use DBCC FLUSHPROCINDB. All of these commands are for testing purposes and should not be run on a production SQL Server. [7.0, 2000] <EM>Updated 2-7-2003</EM></P><P class=asterisk>*****</P><P><STRONG>Orphan SQL Server sessions can negatively affect SQL Server's performance</STRONG>. An orphan SQL Server session can occur when a client improperly disconnects from SQL Server, such as when the client looses power. When this happens, the client cannot tell SQL Server to properly close the connection, so the SQL Server connection remains open, even though it is not being used.</P><P>This can affect SQL Server's performance two ways. First, they use up SQL Server connections, which takes up server resources. Secondly, it is possible that the orphan connections may be holding locks that block other users; or temp tables or cursors may be held open that also take up unnecessary server resources.</P><P>The OS periodically checks for inactive SQL Server sessions, and if it finds any, it will notify SQL Server so that the connection can be removed. Unfortunately, this only occurs every 1-2 hours, depending on the protocol used. If orphaned SQL Server sessions become a problem, Windows Server's registry can be modified so that it checks more often for orphaned connections.</P><P>Identifying an orphaned connection from SQL Server is very difficult, but if you can identify it, it can be removed by KILLing the connection using Enterprise Manager or Query Analyzer. [6.5, 7.0, 2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P><STRONG>If your application opens a DB-Library connection to SQL Server 7.0 and it uses either the TCP/IP or IPX/SPX network libraries</STRONG>, and you are using SQL Server 7.0 with no service pack or Service Pack 1, then SQL Server will experience severe memory leaks each time a new connection is opened, that can significantly affect its performance. To resolve this problem, the best solution is to install Service Pack 2, which will correct the problem. If you cannot install the service pack, then another option is to use another network library, such as Named Pipes or Multiprotocol. [7.0] <A href="http://support.microsoft.com/support/kb/articles/Q236/4/39.ASP">More from Microsoft</A> <EM>Added 11-15-2000</EM></P><P class=asterisk>*****</P><P>For best performance, <STRONG>don't mix production databases and development (test or staging) databases on the same physical server</STRONG>. This not only serves to better separate the two functions (production and development), but prevents developers from using up server resources that could be better used by production users. [6.5, 7.0, 2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P>When we think of performance, we usually think about speeding up our application's performance. But <STRONG>another way to look at performance is to look at our performance as DBA's or Transact-SQL developers</STRONG>.</P><P>For example, one of the easiest ways to speed up our Transact-SQL coding, in addition to making it easier to maintain and troubleshoot our code once it is written, is to format it in an easy to read format.</P><P>While there are many different code formatting guidelines available, here are some basic ones you should consider following, if you aren't doing so already:</P><UL><LI>Begin each line of your Transact-SQL code with a SQL verb, and capitalize all Transact-SQL statements and clauses, such as: </LI></UL><BLOCKQUOTE><P class=textCode>SELECT customer_number, customer_name <BR>FROM customer <BR>WHERE customer_number &gt; 1000 <BR>ORDER BY customer_number</P></BLOCKQUOTE><UL><LI>If a line of Transact-SQL code is too long to fit onto one line, indent the following line(s), such as: </LI></UL><BLOCKQUOTE><P class=textCode>SELECT customer_number, customer_name, customer_address, <BR>customer_state, customer_zip, customer_phonenumber</P></BLOCKQUOTE><UL><LI>Separate logical groupings of Transact-SQL code by using appropriate comments and documentation explaining what each grouping goes. </LI></UL><P>These are just a few of the many possible guidelines you can follow when writing your Transact-SQL code to make it more readable by you and others. You just need to decide on some standard, and then always follow it in your coding. If you do this, you will definitely boost your coding performance. [6.5, 7.0, 2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P><STRONG>Be wary of allowing users to directly access your databases (especially OLTP databases) with third-party database access tools</STRONG>, such as Microsoft Excel or Access. Many of these tools can wreck havoc with your database's performance. Here are some reasons why:</P><UL><LI>Often these users aren't experienced using these tools, and create overly complex queries that eat up server resources. At the other extreme, their queries may not be complex enough (such as lacking effective WHERE clauses) and return thousands, if not millions, or unnecessary rows of data. <LI>This reporting activity can often lock rows, pages, or tables, creating user contention for data and reducing the database's performance. <LI>These tools are often file-based. This means that even if an effective query is written, the entire table (or multiple tables in the case of joins) have to be returned to the client software where the query is actually performed, not at the server. This can not only lead to excess server activity, but it can play havoc on your network. </LI></UL><P>If you have no choice but to allow users access to your data, try to avoid them hitting your production OLTP databases. Instead, point them to a "reporting" server that has been replicated, or is in the form of a datamart or data warehouse. [6.5, 7.0, 2000] <EM>Updated 9-8-2004</EM></P><P class=asterisk>*****</P><P>SQL Server 2000 offers support of SSL encryption between clients and the server. While selecting this option prevents the data from being viewed, it also adds additional overhead and reduces performance. <STRONG>Only use SSL encryption if absolutely required</STRONG>. If you need to use SSL encryption, consider purchasing a SSL encryption processor for the server to speed performance. [2000] <EM>Updated 9-8-2004</EM></P><P class=asterisk>*****</P><P><STRONG>SQL Server 2000 supports named instances of SQL Server</STRONG>. For example, this feature allows you to run both SQL Server 6.5 and SQL Server 2000 on the same server; or to run SQL Server 7.0 and SQL Server 2000 on the same server; or to run SQL Server 6.5, SQL Server 7.0, and SQL Server 2000 on the same server; up to 16 concurrent instances of SQL Server 2000 on the same server.</P><P>As you might imagine, each running instance of SQL Server takes up server resources. Although some resources are shared by multiple running instances, such as MSDTC and the Microsoft Search services, most are not. Because of this, each additional instance of SQL Server running on the same server have to fight for available resources, hurting performance.</P><P>For best performance, run only a single instance (usually the default) on a single physical server. The main reasons for using named instances is for upgrading older versions of SQL Server to SQL Server 2000, transition periods where you need to test your applications on multiple versions of SQL Server, and for use on developer's workstations. [2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P>If you run the <STRONG>ALTER TABLE DROP COLUMN</STRONG> statement to drop a variable length or text column, did you know that SQL Server will not automatically reclaim this space after performing this action? To reclaim this space, which will help to reduce unnecessary I/O due to the wasted space, you can run the following command, which is new to SQL Server 2000.</P><P class=textCode>DBCC CLEANTABLE (database_name, table_name)</P><P>Before running this command, you will want to read about it in Books Online to learn about some of its options that may be important to you. [2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P><STRONG>Trace flags, which are used to enable and disable some special database functions temporarily, can sometimes chew up CPU utilization and other resources on your SQL Server unnecessarily</STRONG>. If you just use them for a short time to help diagnose a problem, for example, and then turn them off as soon as your are done using them, then the performance hit you experience is small and temporary.</P><P>What happens sometimes is that you, or another DBA, turns on a trace flag, but forgets to turn it off. This of course, can negatively affect your SQL Server's performance. If you want to check to see if there are any trace flags turned on on a SQL Server, run this command in Query Analyzer:</P><P class=textCode>DBCC TRACESTATUS(-1)</P><P>If there are any trace flags on, you will see them listed on the screen after running this command. DBCC TRACESTATUS only finds traces created at the client (connection) level. If a trace has been turned on for an entire server, this will not show up.</P><P>If you find any, you can turn them off using this command:</P><P class=textCode>DBCC TRACEOFF(<EM>number of trace</EM>)</P><P>[7.0, 2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P>SQL Server offers a feature called the <STRONG>black box</STRONG>. When enabled, the black box creates a trace file of the last 128K worth of queries and exception errors. This can be a great tool for troubleshooting some SQL Server problems, such as crashes.</P><P>Unfortunately, this feature uses up SQL Server resources to maintain the trace file than can negatively affect its performance. Generally, you will want to only turn the black box on when troubleshooting, and turn it off during normal production. This way, your SQL Server will be minimally affected. [7.0, 2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P><STRONG>If you have ever performed a SELECT COUNT(*) on a very large table, you know how long it can take</STRONG>. For example, when I ran the following command on a large table I manage:</P><P class=textCode>SELECT COUNT(*) from &lt;table_name&gt;</P><P>It took 1:09 to count 10,725,948 rows in the table. At the same time, SQL Server had to perform a lot of logical and physical I/O in order to perform the count, chewing up important SQL Server resources.</P><P>A much faster, and more efficient, way of counting rows in a table is to run the following query:</P><P class=textCode>SELECT rows <BR>FROM sysindexes <BR>WHERE id = OBJECT_ID('&lt;table_name&gt;') AND indid &lt; 2</P><P>When I run the query against the same table, it takes less than a second to run, and it gave me the same results. Not a bad improvement, and it took virtually no I/O. This is because the row count of your tables is stored in the sysindexes system table of your database. So instead of counting rows when you need to, just look up the row count in the sysindexes table.</P><P>The is one potential downside to using the sysindexes table. And that this system table is not updated in real time, so it might underestimate the number of rows you actually have. Assuming you have the database option turned on to "Auto Create Statistics" and "Auto Update Statistics", the value you get should be very close to being correct, if not correct. If you can live with a very close estimate, then this is the best way to count rows in your tables. [7.0, 2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P><STRONG>Looking for some new tools to help performance tune your operating system?</STRONG> Then check out the performance tools at <A href="http://www.sysinternals.com/">Sysinternals</A>. For example, they have tools to defrag your server's swap file, among many others. And best of all, most are free. [6.5, 7.0, 2000] <EM>Updated 9-6-2004</EM></P><P class=asterisk>*****</P><P><STRONG>Do you use Enterprise Manager to access remote servers, possibly over a slow WAN link?</STRONG> If you do, have you ever had any problems getting Enterprise Manager to connect to the remote server?</P><P>If so, the problem may lay in the fact that if Enterprise Manager cannot make a connection within 4 seconds, then the connection attempt fails. To overcome slow network connections, you can change the default Enterprise Manager timeout value from 4 seconds to any amount of time you like.</P><P>To change the default timeout value, select Tools|Options from Enterprise Manager, and then select the "Connection" tab if you have SQL Server 7.0, or the "Advanced" tab if you have SQL Server 2000. Here, change the "Login time-out (seconds)" option to a higher number. [7.0, 2000] <EM>Updated 10-18-2004</EM></P><P class=asterisk>*****</P><P><STRONG>SQLDIAG.exe is a command line tools that collects information</p><h3>Notes</h3><p></p>